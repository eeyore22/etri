{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os \n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing before applying imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, make sure that all raw hourly files have the same features.\n",
    "# If not, add nan\n",
    "\n",
    "desired_columns = ['timestamp', 'hr', 'magnitude_mAcc', 'bvp_positive', \n",
    "                       'bvp_negative', 'temp', 'magnitude_e4Acc', 'lat', 'lon', 'accuracy',\n",
    "                       'magnitude_mMag', 'eda', 'magnitude_mGyr']\n",
    "\n",
    "directory = \"./train_dataset/sensor_data/hourly_remade\"\n",
    "#desired_columns = ['timestamp', 'action','condition', 'place', 'emotionPositive', 'emotionTension', 'activity']                   \n",
    "#directory = \"./train_dataset/sensor_data/labels\"\n",
    "\n",
    "# Iterate through the files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if 'hourly' in filename:\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        df = pd.read_csv(filepath)\n",
    "        for col in desired_columns:\n",
    "            if col not in df.columns:\n",
    "                df[col] = ''\n",
    "        \n",
    "        userId = filename.split('_hourly.csv')[0]\n",
    "        df['userId'] = userId\n",
    "        df['date'] = pd.to_datetime(df['timestamp']).dt.date\n",
    "        df = df[['userId', 'date'] + desired_columns]\n",
    "        # Save the modified DataFrame to the same file\n",
    "        df.to_csv(filepath, index=False)\n",
    "\n",
    "print(\"Files have been processed and saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label's emotion features -> sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories containing the CSV files\n",
    "hourly_directory = './train_dataset/sensor_data/hourly_remade'\n",
    "labels_directory = './train_dataset/sensor_data/labels'\n",
    "\n",
    "# Columns to move\n",
    "columns_to_move = ['emotionPositive', 'emotionTension']\n",
    "\n",
    "# List all files in the hourly and labels directories\n",
    "hourly_files = [file for file in os.listdir(hourly_directory) if file.endswith('.csv')]\n",
    "labels_files = [file for file in os.listdir(labels_directory) if file.endswith('.csv')]\n",
    "\n",
    "# Function to extract user ID from filename\n",
    "def extract_user_id(filename):\n",
    "    return filename.split('_')[0]\n",
    "\n",
    "# Create a dictionary to map user IDs to their corresponding label files\n",
    "labels_map = {extract_user_id(file): file for file in labels_files}\n",
    "\n",
    "# Process each hourly file\n",
    "for hourly_file in hourly_files:\n",
    "    user_id = extract_user_id(hourly_file)\n",
    "    if user_id in labels_map:\n",
    "        hourly_file_path = os.path.join(hourly_directory, hourly_file)\n",
    "        label_file_path = os.path.join(labels_directory, labels_map[user_id])\n",
    "        \n",
    "        # Load the CSV files\n",
    "        hourly_df = pd.read_csv(hourly_file_path)\n",
    "        label_df = pd.read_csv(label_file_path)\n",
    "        \n",
    "        # Check if the columns to move exist in the label file\n",
    "        if all(column in label_df.columns for column in columns_to_move):\n",
    "            # Move the specified columns\n",
    "            hourly_df[columns_to_move] = label_df[columns_to_move]\n",
    "            \n",
    "            # Remove the specified columns from the label DataFrame\n",
    "            label_df.drop(columns=columns_to_move, inplace=True)\n",
    "        \n",
    "            # Save the modified hourly DataFrame back to the CSV file\n",
    "            hourly_df.to_csv(hourly_file_path, index=False)\n",
    "            \n",
    "            # Save the modified label DataFrame back to the CSV file\n",
    "            label_df.to_csv(label_file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAITS imputer for numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pypots.imputation import SAITS\n",
    "from pypots.utils.metrics import calc_mae\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def mcar(data, missing_rate):\n",
    "    np.random.seed(0)\n",
    "    data_with_nan = data.copy()\n",
    "    missing_mask = np.random.rand(*data.shape) < missing_rate\n",
    "    data_with_nan[missing_mask] = np.nan\n",
    "    return data_with_nan\n",
    "\n",
    "def impute_SAITS(path):\n",
    "    data = pd.read_csv(path)\n",
    "    \n",
    "    if 'timestamp' not in data.columns:\n",
    "        raise KeyError(\"The 'timestamp' column is not found in the dataset.\")\n",
    "    \n",
    "    time_column = 'timestamp'\n",
    "    #additional_columns = ['userId', 'date']\n",
    "    #feature_columns = data.columns.drop([time_column] + additional_columns)\n",
    "    feature_columns = data.columns.drop([time_column])\n",
    "\n",
    "    # Separate numeric and non-numeric columns\n",
    "    numeric_columns = data[feature_columns].select_dtypes(include=[np.number]).columns\n",
    "    non_numeric_columns = data[feature_columns].select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "    # Data preprocessing\n",
    "    num_samples = data[time_column].nunique()\n",
    "    data_numeric = data[numeric_columns]\n",
    "    data_non_numeric = data[non_numeric_columns]\n",
    "    \n",
    "    # Standardize numeric data\n",
    "    scaler = StandardScaler()\n",
    "    data_numeric_scaled = pd.DataFrame(scaler.fit_transform(data_numeric), columns=numeric_columns)\n",
    "    \n",
    "    # Apply missing values to numeric data\n",
    "    X_numeric = data_numeric_scaled.to_numpy().reshape(num_samples, -1, len(numeric_columns))\n",
    "    X_numeric_ori = X_numeric  # Keep original numeric data for validation\n",
    "    X_numeric = mcar(X_numeric, 0.1)  # Randomly hold out 10% observed values as ground truth\n",
    "    dataset_numeric = {\"X\": X_numeric}\n",
    "    print(X_numeric.shape)  # (num_samples, 1, len(numeric_columns))\n",
    "\n",
    "    # Model training\n",
    "    saits = SAITS(\n",
    "        n_steps=X_numeric.shape[1], \n",
    "        n_features=X_numeric.shape[2], \n",
    "        n_layers=2, \n",
    "        d_model=256, \n",
    "        d_ffn=128, \n",
    "        n_heads=4, \n",
    "        d_k=64, \n",
    "        d_v=64, \n",
    "        dropout=0.1, \n",
    "        epochs=10\n",
    "    )\n",
    "\n",
    "    # Use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\n",
    "    saits.fit(dataset_numeric)\n",
    "    imputation = saits.impute(dataset_numeric)  # Impute the originally-missing values and artificially-missing values\n",
    "    indicating_mask = np.isnan(X_numeric) ^ np.isnan(X_numeric_ori)  # Indicating mask for imputation error calculation\n",
    "    mae = calc_mae(imputation, np.nan_to_num(X_numeric_ori), indicating_mask)  # Calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "\n",
    "    print(f\"Mean Absolute Error for numeric data: {mae}\")\n",
    "\n",
    "    # Convert imputed data back to DataFrame\n",
    "    imputed_numeric_data = imputation.reshape(-1, len(numeric_columns))\n",
    "    imputed_numeric_df = pd.DataFrame(imputed_numeric_data, columns=numeric_columns)\n",
    "    imputed_numeric_df = pd.DataFrame(scaler.inverse_transform(imputed_numeric_df), columns=numeric_columns)\n",
    "\n",
    "    # Check if there are non-numeric columns and impute them using mode imputation grouped by 'date'\n",
    "    if len(non_numeric_columns) > 0:\n",
    "        imputed_non_numeric_df = data_non_numeric.copy()\n",
    "        \n",
    "        for col in non_numeric_columns:\n",
    "            def mode_impute(series):\n",
    "                mode_val = series.mode()\n",
    "                if mode_val.empty:\n",
    "                    return series\n",
    "                return series.fillna(mode_val.iloc[0])\n",
    "            \n",
    "            imputed_non_numeric_df[col] = data.groupby('date')[col].transform(mode_impute)\n",
    "        \n",
    "        # Handle any remaining missing values\n",
    "        for col in non_numeric_columns:\n",
    "            if imputed_non_numeric_df[col].isnull().any():\n",
    "                print(f\"Handling remaining missing values in column: {col}\")\n",
    "                fallback_imputer = SimpleImputer(strategy='most_frequent')\n",
    "                imputed_non_numeric_df[col] = fallback_imputer.fit_transform(imputed_non_numeric_df[[col]])\n",
    "        \n",
    "        imputed_df = pd.concat([imputed_numeric_df, imputed_non_numeric_df], axis=1)\n",
    "    else:\n",
    "        imputed_df = imputed_numeric_df\n",
    "\n",
    "    # Add back 'userId', 'date', and 'timestamp' columns\n",
    "    #imputed_df['userId'] = data['userId']\n",
    "    #imputed_df['date'] = data['date']\n",
    "    imputed_df['timestamp'] = data['timestamp']\n",
    "\n",
    "    return imputed_df\n",
    "\n",
    "\n",
    "path = \"./train_dataset/sensor_data/hourly_remade/user30_hourly.csv\"\n",
    "try:\n",
    "    imputed_df = impute_SAITS(path)\n",
    "    #import ace_tools as tools; tools.display_dataframe_to_user(name=\"Imputed Data\", dataframe=imputed_df)\n",
    "    print(imputed_df.head())\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "\n",
    "#imputed_df.to_csv('./train_dataset/sensor_data_imputed/user01_sensor_imputed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mode imputer for non-numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_by_mode(df, group_col, cols_to_impute):\n",
    "    for col in cols_to_impute:\n",
    "        mode_func = lambda x: x.mode().iloc[0] if not x.mode().empty else x\n",
    "        df[col] = df.groupby(group_col)[col].transform(lambda x: x.fillna(mode_func(x)))\n",
    "    return df\n",
    "\n",
    "def impute_missing_values(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    columns_to_impute = ['action', 'condition', 'place', 'activity']\n",
    "    imputed_df = impute_by_mode(df, 'date', columns_to_impute)\n",
    "    \n",
    "    # Ensure all missing values are filled\n",
    "    for col in columns_to_impute:\n",
    "        if imputed_df[col].isnull().any():\n",
    "            overall_mode = imputed_df[col].mode().iloc[0]\n",
    "            imputed_df[col].fillna(overall_mode, inplace=True)\n",
    "    \n",
    "    return imputed_df\n",
    "\n",
    "# Directories\n",
    "input_dir = './train_dataset/sensor_data/labels'\n",
    "output_dir = './train_dataset/sensor_data_imputed'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Load and impute missing values\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        imputed_df = impute_missing_values(file_path)\n",
    "        \n",
    "        # Extract userId from the filename\n",
    "        user_id = imputed_df['userId'].iloc[0]\n",
    "        \n",
    "        # Save the imputed DataFrame\n",
    "        output_file_path = os.path.join(output_dir, f'{user_id}_state_imputed.csv')\n",
    "        imputed_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Imputation and saving completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "input_dir = './train_dataset/sensor_data_imputed'\n",
    "output_dir = './train_dataset/sensor_data_merged'\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to merge sensor and state files for each user\n",
    "def merge_files(sensor_file, state_file):\n",
    "    sensor_df = pd.read_csv(os.path.join(input_dir, sensor_file))\n",
    "    state_df = pd.read_csv(os.path.join(input_dir, state_file))\n",
    "    \n",
    "    # Drop 'Unnamed: 0' column if it exists\n",
    "    sensor_df = sensor_df.loc[:, ~sensor_df.columns.str.contains('^Unnamed')]\n",
    "    state_df = state_df.loc[:, ~state_df.columns.str.contains('^Unnamed')]\n",
    "    \n",
    "    merged_df = pd.merge(sensor_df, state_df, on=['userId', 'timestamp'])\n",
    "    \n",
    "    # Handle 'date_x' and 'date_y' columns\n",
    "    if 'date_x' in merged_df.columns and 'date_y' in merged_df.columns:\n",
    "        merged_df['date'] = merged_df['date_x'].combine_first(merged_df['date_y'])\n",
    "        merged_df.drop(columns=['date_x', 'date_y'], inplace=True)\n",
    "    elif 'date_x' in merged_df.columns:\n",
    "        merged_df.rename(columns={'date_x': 'date'}, inplace=True)\n",
    "    elif 'date_y' in merged_df.columns:\n",
    "        merged_df.rename(columns={'date_y': 'date'}, inplace=True)\n",
    "    \n",
    "    # Ensure the correct column order\n",
    "    column_order = [\n",
    "        'userId', 'date', 'timestamp','hr', 'magnitude_mAcc', 'bvp_positive', 'bvp_negative', 'temp', \n",
    "        'magnitude_e4Acc', 'lat', 'lon', 'accuracy', 'magnitude_mMag', 'eda', \n",
    "        'magnitude_mGyr', 'emotionPositive', 'emotionTension', 'action', 'condition', 'place', 'activity'\n",
    "    ]\n",
    "    \n",
    "    # Ensure that columns not in column_order are added at the end\n",
    "    merged_df = merged_df[[col for col in column_order if col in merged_df.columns] + \n",
    "                          [col for col in merged_df.columns if col not in column_order]]\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Get list of files in the input directory\n",
    "files = os.listdir(input_dir)\n",
    "\n",
    "# Group files by userId\n",
    "file_groups = {}\n",
    "for filename in files:\n",
    "    if filename.endswith('.csv'):\n",
    "        user_id = filename.split('_')[0]\n",
    "        if user_id not in file_groups:\n",
    "            file_groups[user_id] = {'sensor': None, 'state': None}\n",
    "        if 'sensor' in filename:\n",
    "            file_groups[user_id]['sensor'] = filename\n",
    "        elif 'state' in filename:\n",
    "            file_groups[user_id]['state'] = filename\n",
    "\n",
    "# Process each group of files\n",
    "for user_id, file_pair in file_groups.items():\n",
    "    if file_pair['sensor'] and file_pair['state']:\n",
    "        merged_df = merge_files(file_pair['sensor'], file_pair['state'])\n",
    "        \n",
    "        # Save the merged DataFrame\n",
    "        output_file_path = os.path.join(output_dir, f'{user_id}_imputed.csv')\n",
    "        merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(\"Merging and saving completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = './train_dataset/sensor_data_merged'\n",
    "\n",
    "# List all CSV files in the directory\n",
    "csv_files = [f for f in os.listdir(directory) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize an empty list to store dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Read each CSV file and append the dataframe to the list\n",
    "for csv_file in csv_files:\n",
    "    file_path = os.path.join(directory, csv_file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    dataframes.append(df)\n",
    "\n",
    "# Concatenate all dataframes into a single dataframe\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "merged_df.rename(columns={'hr':'heart_rate', 'lat':'latitude', 'lon':'longitude'}, inplace=True)\n",
    "merged_df.to_csv(\"./combined_after_imputation.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pygrinder import mcar\n",
    "import SAITS\n",
    "from pypots.utils.metrics import calc_mae\n",
    "\n",
    "# load data\n",
    "#file_path = \"./train_dataset/sensor_data_imputed/combined_sensor_data.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# split df into time_column and features_columns\n",
    "time_column = 'timestamp'  \n",
    "feature_columns = data.columns.drop(time_column)\n",
    "\n",
    "# data preprocessing\n",
    "num_samples = data[time_column].nunique()\n",
    "data = data.drop([time_column], axis=1)\n",
    "X = data.to_numpy().reshape(num_samples, -1, len(feature_columns))\n",
    "X_ori = X  # keep X_ori for validation\n",
    "X = mcar(X, 0.1)  # randomly hold out 10% observed values as ground truth\n",
    "dataset = {\"X\": X}\n",
    "print(X.shape)  # (684, 1, 13)\n",
    "\n",
    "\n",
    "# model training\n",
    "saits = SAITS(n_steps=X.shape[1], n_features=X.shape[2], n_layers=2, d_model=256, d_ffn=128, n_heads=4, d_k=64, d_v=64, dropout=0.1, epochs=10)\n",
    "\n",
    "# here we use the whole dataset as the training set because ground truth is not visible to the model, you can also split it into train/val/test sets\n",
    "saits.fit(dataset)\n",
    "imputation = saits.impute(dataset)  # impute the originally-missing values and artificially-missing values\n",
    "indicating_mask = np.isnan(X) ^ np.isnan(X_ori)  # indicating mask for imputation error calculation\n",
    "mae = calc_mae(imputation, np.nan_to_num(X_ori), indicating_mask)  # calculate mean absolute error on the ground truth (artificially-missing values)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "# convert imputed data back to DataFrame\n",
    "imputed_data = imputation.reshape(-1, len(feature_columns))\n",
    "imputed_df = pd.DataFrame(imputed_data, columns=feature_columns)\n",
    "\n",
    "imputed_df.drop(columns=['index'], inplace=True)\n",
    "#imputed_df.to_csv(\"./train_dataset/sensor_data_imputed/combined_sensor_data_imputed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "density plot before and after train imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_distributions(before_path, after_path, n_cols=3):\n",
    "    original_data = pd.read_csv(before_path)\n",
    "    imputed_data = pd.read_csv(after_path)\n",
    "    \n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_columns = original_data.select_dtypes(include=[np.number]).columns\n",
    "    categorical_columns = original_data.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    # Plot numeric columns\n",
    "    n_rows_numeric = int(np.ceil(len(numeric_columns) / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows_numeric, n_cols, figsize=(15, 5 * n_rows_numeric))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, column in enumerate(numeric_columns):\n",
    "        original_col = original_data[column].dropna().values\n",
    "        imputed_col = imputed_data[column].dropna().values\n",
    "\n",
    "        sns.kdeplot(original_col, ax=axes[i], label='Original', fill=False, alpha=0.5)\n",
    "        sns.kdeplot(imputed_col, ax=axes[i], label='Imputed', fill=False, alpha=0.5)\n",
    "        axes[i].set_title(column)\n",
    "        axes[i].legend()\n",
    "\n",
    "    for ax in axes[len(numeric_columns):]:\n",
    "        ax.remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot categorical columns\n",
    "    n_rows_categorical = int(np.ceil(len(categorical_columns) / n_cols))\n",
    "    fig, axes = plt.subplots(n_rows_categorical, n_cols, figsize=(15, 5 * n_rows_categorical))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, column in enumerate(categorical_columns):\n",
    "        original_counts = original_data[column].value_counts()\n",
    "        imputed_counts = imputed_data[column].value_counts()\n",
    "\n",
    "        width = 0.35  # width of the bars\n",
    "        original_counts.plot(kind='bar', ax=axes[i], position=0, width=width, label='Original', color='blue', alpha=0.5)\n",
    "        imputed_counts.plot(kind='bar', ax=axes[i], position=1, width=width, label='Imputed', color='orange', alpha=0.5)\n",
    "\n",
    "        axes[i].set_title(column)\n",
    "        axes[i].legend()\n",
    "\n",
    "    for ax in axes[len(categorical_columns):]:\n",
    "        ax.remove()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# File paths\n",
    "before_imputation_path = './combined_before_imputation.csv'\n",
    "after_imputation_path = './combined_after_imputation.csv'\n",
    "\n",
    "# Plot the density distributions before and after imputation\n",
    "plot_density_distributions(before_imputation_path, after_imputation_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-build train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_state_imputed = pd.read_csv('./combined_after_imputation.csv')\n",
    "#sensor_state_original = pd.read_csv('./combined_before_imputation.csv')\n",
    "\n",
    "survey = pd.read_csv(\"./train_dataset/user_survey_2020_transformed.csv\")\n",
    "columns_to_replace = ['caffeine', 'cAmount(ml)', 'alcohol', 'aAmount(ml)']\n",
    "survey[columns_to_replace] = survey[columns_to_replace].fillna('unknown')\n",
    "survey.drop(columns=['amPm', 'startInput', 'endInput'], inplace=True)\n",
    "labels = pd.read_csv(\"./train_dataset/train_label.csv\")\n",
    "sleep = pd.read_csv('./train_dataset/user_sleep_2020.csv')\n",
    "sleep.drop(columns=[ 'timezone', 'startDt', 'endDt', 'lastUpdate'], inplace=True)\n",
    "\n",
    "labels.rename(columns={'subject_id':'userId'}, inplace=True)\n",
    "labels.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "labels_sensor_state_merged = pd.merge(labels, sensor_state_imputed, on=['userId', 'date'], how='left')\n",
    "labels_sensor_state_merged['date'].nunique()\n",
    "labels_sensor_state_survey_merged = pd.merge(labels_sensor_state_merged, survey, on=['userId', 'date'], how='left')\n",
    "labels_sensor_state_survey_merged['date'].nunique()\n",
    "all_merged = pd.merge(labels_sensor_state_survey_merged, sleep, on=['userId', 'date'], how='left')\n",
    "all_merged['date'].nunique()\n",
    "all_merged.drop(columns=['Unnamed: 0', 'hr_min', 'hr_max', 'rr_min', 'rr_max'], inplace=True)\n",
    "all_merged = all_merged.dropna(subset=['timestamp']) # judged as corrupted so dropped these"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impute once again for the post-merging dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_by_mode(df, group_cols, cols_to_impute):\n",
    "    for col in cols_to_impute:\n",
    "        mode_func = lambda x: x.mode().iloc[0] if not x.mode().empty else x\n",
    "        df[col] = df.groupby(group_cols)[col].transform(lambda x: x.fillna(mode_func(x)))\n",
    "    return df\n",
    "\n",
    "def impute_nonnumeric_missing_values(df):\n",
    "    columns_to_impute = ['action', 'condition', 'place', 'activity']\n",
    "    group_cols = ['userId', 'date']\n",
    "    imputed_df = impute_by_mode(df, group_cols, columns_to_impute)\n",
    "    \n",
    "    # Ensure all missing values are filled\n",
    "    for col in columns_to_impute:\n",
    "        if imputed_df[col].isnull().any():\n",
    "            overall_mode = imputed_df[col].mode().iloc[0]\n",
    "            imputed_df[col].fillna(overall_mode, inplace=True)\n",
    "    \n",
    "    return imputed_df\n",
    "\n",
    "\n",
    "all_merged = impute_nonnumeric_missing_values(all_merged)\n",
    "#all_merged[all_merged['condition'].isna()]\n",
    "\n",
    "def impute_by_median(df, group_cols, cols_to_impute):\n",
    "    for col in cols_to_impute:\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            median_func = lambda x: x.median()\n",
    "            df[col] = df.groupby(group_cols)[col].transform(lambda x: x.fillna(median_func(x)))\n",
    "    \n",
    "    # Ensure all missing values are filled\n",
    "    for col in cols_to_impute:\n",
    "        if df[col].isnull().any():\n",
    "            overall_median = df[col].median()\n",
    "            df[col].fillna(overall_median, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "cols_to_impute = ['heart_rate', 'magnitude_mAcc', 'temp', 'magnitude_e4Acc', 'latitude', 'longitude', 'accuracy', 'magnitude_mMag', 'magnitude_mGyr',\n",
    " 'wakeupduration', 'lightsleepduration', 'deepsleepduration', 'wakeupcount', 'durationtosleep', 'remsleepduration', 'durationtowakeup',\n",
    " 'hr_average', 'rr_average', 'breathing_disturbances_intensity', 'snoring', 'snoringepisodecount', 'sleep_score']\n",
    "group_cols = ['userId', 'date']\n",
    "\n",
    "# Apply the imputation function\n",
    "all_merged = impute_by_median(all_merged, group_cols, cols_to_impute)\n",
    "na_values_after = all_merged.isna().sum()\n",
    "print(f\"NA values {na_values_after}\")\n",
    "\n",
    "all_merged.rename(columns={'Q1':'daily_Q1',\n",
    "                              'Q2':'daily_Q2',\n",
    "                              'Q3':'daily_Q3',\n",
    "                              'S1':'daily_S1',\n",
    "                              'S2':'daily_S2',\n",
    "                              'S3':'daily_S3',\n",
    "                              'S4':'daily_S4'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last feature engineering: encoding categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_merged.select_dtypes(exclude=[int, float])\n",
    "\n",
    "specified_columns = ['action', 'condition', 'place', 'activity', 'caffeine', 'alcohol', 'cAmount(ml)', 'aAmount(ml)']\n",
    "one_hot_encoded_df = pd.get_dummies(all_merged[specified_columns], drop_first=False)\n",
    "one_hot_encoded_df_int = one_hot_encoded_df.astype(int)\n",
    "# Concatenate the one-hot encoded columns back to the original DataFrame, excluding the original columns\n",
    "final_df = pd.concat([all_merged.drop(columns=specified_columns), one_hot_encoded_df_int], axis=1)\n",
    "final_df.head()\n",
    "final_df.to_csv(\"train_imputed.csv\")\n",
    "with open('imputed_train_with_labels.pickle', 'wb') as f:\n",
    "    pickle.dump(final_df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Load the Data and Separate Features and Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./imputed_train_with_labels.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open('./validation_with_labels.pickle', 'rb') as f:\n",
    "    validation_data = pickle.load(f)\n",
    "\n",
    "# Separate features and target labels\n",
    "train_features = train_data.drop(columns=[col for col in train_data.columns if col.startswith('daily_')])\n",
    "train_features.drop(columns=['userId', 'date', 'timestamp'], inplace=True)\n",
    "train_labels = train_data[[col for col in train_data.columns if col.startswith('daily_')]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_features\n",
    "y_train = train_labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize data \n",
    "scaler = StandardScaler()\n",
    "\n",
    "prefixes = ['action', 'condition', 'place', 'activity', 'caffeine', 'alcohol', 'cAmount(ml)', 'aAmount(ml)']\n",
    "\n",
    "# Identify columns that start with the specified prefixes\n",
    "onehot_cols = [col for col in train_features.columns if any(col.startswith(prefix) for prefix in prefixes)]\n",
    "# Convert identified categorical columns to uint8\n",
    "train_features[onehot_cols] = train_features[onehot_cols].astype('uint8')\n",
    "\n",
    "numeric_cols = train_features.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = train_features.select_dtypes(include=['uint8', 'bool']).columns\n",
    "\n",
    "# Standardize numeric columns \n",
    "X_train_numeric_scaled = scaler.fit_transform(X_train[numeric_cols])\n",
    "#X_test_numeric_scaled = scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Convert scaled numeric data back to DataFrame\n",
    "X_train_numeric_scaled_df = pd.DataFrame(X_train_numeric_scaled, columns=numeric_cols, index=X_train.index)\n",
    "#X_test_numeric_scaled_df = pd.DataFrame(X_test_numeric_scaled, columns=numeric_cols, index=X_test.index)\n",
    "\n",
    "# Combine scaled numeric columns with categorical columns\n",
    "X_train_scaled = pd.concat([X_train_numeric_scaled_df, X_train[categorical_cols]], axis=1)\n",
    "#X_test_scaled = pd.concat([X_test_numeric_scaled_df, X_test[categorical_cols]], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Preprocess validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = validation_data.drop(columns=[col for col in validation_data.columns if col.startswith('daily_')])\n",
    "validation_features.drop(columns=['subject_id', 'date', 'timestamp'], inplace=True)\n",
    "validation_labels = validation_data[[col for col in validation_data.columns if col.startswith('daily_')]]\n",
    "\n",
    "val = set(validation_features.columns)\n",
    "train = set(train_features.columns)\n",
    "in_val_not_in_train = val - train\n",
    "only_in_val = list(in_val_not_in_train)\n",
    "#print(f\"In val not in train: {in_val_not_in_train}, In train not in val: {in_train_not_in_val}\")\n",
    "\n",
    "prefixes = ['action', 'condition', 'place', 'activity', 'caffeine', 'alcohol', 'cAmount(ml)', 'aAmount(ml)']\n",
    "onehot_cols = [col for col in train_features.columns if any(col.startswith(prefix) for prefix in prefixes)]\n",
    "# Identify numeric columns (those that are not one-hot encoded)\n",
    "numeric_cols = train_features.columns.difference(onehot_cols).tolist()\n",
    "\n",
    "# Identify features that are missing in the validation set\n",
    "# Assuming features is the DataFrame used for training which contains all columns\n",
    "missing_features = [col for col in train_features.columns if col not in validation_features.columns]\n",
    "\n",
    "# Append the missing features to the validation set with placeholder values (e.g., NaN)\n",
    "for feature in missing_features:\n",
    "    validation_features[feature] = float('nan')\n",
    "\n",
    "X_missing = validation_features.drop(columns=only_in_val)\n",
    "for feature in missing_features:\n",
    "    X_missing[feature] = float('nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below two blocks are for aligning the column orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_missing = X_missing[['heart_rate',\n",
    " 'magnitude_mAcc',\n",
    " 'bvp_positive',\n",
    " 'bvp_negative',\n",
    " 'temp',\n",
    " 'magnitude_e4Acc',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'accuracy',\n",
    " 'magnitude_mMag',\n",
    " 'eda',\n",
    " 'magnitude_mGyr',\n",
    " 'emotionPositive',\n",
    " 'emotionTension',\n",
    " 'sleep',\n",
    " 'sleepProblem',\n",
    " 'dream',\n",
    " 'amCondition',\n",
    " 'amEmotion',\n",
    " 'pmEmotion',\n",
    " 'pmStress',\n",
    " 'pmFatigue',\n",
    " 'wakeupduration',\n",
    " 'lightsleepduration',\n",
    " 'deepsleepduration',\n",
    " 'wakeupcount',\n",
    " 'durationtosleep',\n",
    " 'remsleepduration',\n",
    " 'durationtowakeup',\n",
    " 'hr_average',\n",
    " 'rr_average',\n",
    " 'breathing_disturbances_intensity',\n",
    " 'snoring',\n",
    " 'snoringepisodecount',\n",
    " 'sleep_score',\n",
    " 'action_care_housemem',\n",
    " 'action_community_interaction',\n",
    " 'action_entertainment',\n",
    " 'action_hobby',\n",
    " 'action_household',\n",
    " 'action_meal',\n",
    " 'action_outdoor_act',\n",
    " 'action_personal_care',\n",
    " 'action_recreation_etc',\n",
    " 'action_recreation_media',\n",
    " 'action_shop',\n",
    " 'action_sleep',\n",
    " 'action_socialising',\n",
    " 'action_study',\n",
    " 'action_travel',\n",
    " 'action_work',\n",
    " 'condition_ALONE',\n",
    " 'condition_WITH_MANY',\n",
    " 'condition_WITH_ONE',\n",
    " 'place_home',\n",
    " 'place_other_indoor',\n",
    " 'place_outdoor',\n",
    " 'place_restaurant',\n",
    " 'place_workplace',\n",
    " 'activity_IN_VEHICLE',\n",
    " 'activity_ON_FOOT',\n",
    " 'activity_STILL',\n",
    " 'activity_UNKNOWN',\n",
    " 'caffeine_caffeinated drink',\n",
    " 'caffeine_coffee',\n",
    " 'caffeine_coke',\n",
    " 'caffeine_tea',\n",
    " 'caffeine_unknown',\n",
    " 'alcohol_beer',\n",
    " 'alcohol_beer&rice wine',\n",
    " 'alcohol_not specified',\n",
    " 'alcohol_soju',\n",
    " 'alcohol_soju&beer',\n",
    " 'alcohol_unknown',\n",
    " 'alcohol_wine',\n",
    " 'cAmount(ml)_100.0',\n",
    " 'cAmount(ml)_150.0',\n",
    " 'cAmount(ml)_200.0',\n",
    " 'cAmount(ml)_250.0',\n",
    " 'cAmount(ml)_260.0',\n",
    " 'cAmount(ml)_280.0',\n",
    " 'cAmount(ml)_300.0',\n",
    " 'cAmount(ml)_350.0',\n",
    " 'cAmount(ml)_355.0',\n",
    " 'cAmount(ml)_360.0',\n",
    " 'cAmount(ml)_400.0',\n",
    " 'cAmount(ml)_450.0',\n",
    " 'cAmount(ml)_500.0',\n",
    " 'cAmount(ml)_560.0',\n",
    " 'cAmount(ml)_600.0',\n",
    " 'cAmount(ml)_700.0',\n",
    " 'cAmount(ml)_750.0',\n",
    " 'cAmount(ml)_900.0',\n",
    " 'cAmount(ml)_1000.0',\n",
    " 'cAmount(ml)_1500.0',\n",
    " 'cAmount(ml)_unknown',\n",
    " 'aAmount(ml)_200.0',\n",
    " 'aAmount(ml)_250.0',\n",
    " 'aAmount(ml)_300.0',\n",
    " 'aAmount(ml)_330.0',\n",
    " 'aAmount(ml)_400.0',\n",
    " 'aAmount(ml)_500.0',\n",
    " 'aAmount(ml)_600.0',\n",
    " 'aAmount(ml)_700.0',\n",
    " 'aAmount(ml)_720.0',\n",
    " 'aAmount(ml)_750.0',\n",
    " 'aAmount(ml)_800.0',\n",
    " 'aAmount(ml)_900.0',\n",
    " 'aAmount(ml)_1000.0',\n",
    " 'aAmount(ml)_1500.0',\n",
    " 'aAmount(ml)_2000.0',\n",
    " 'aAmount(ml)_3000.0',\n",
    " 'aAmount(ml)_3500.0',\n",
    " 'aAmount(ml)_4000.0',\n",
    " 'aAmount(ml)_unknown']]\n",
    "\n",
    "X_train_scaled = X_train_scaled[['heart_rate',\n",
    " 'magnitude_mAcc',\n",
    " 'bvp_positive',\n",
    " 'bvp_negative',\n",
    " 'temp',\n",
    " 'magnitude_e4Acc',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'accuracy',\n",
    " 'magnitude_mMag',\n",
    " 'eda',\n",
    " 'magnitude_mGyr',\n",
    " 'emotionPositive',\n",
    " 'emotionTension',\n",
    " 'sleep',\n",
    " 'sleepProblem',\n",
    " 'dream',\n",
    " 'amCondition',\n",
    " 'amEmotion',\n",
    " 'pmEmotion',\n",
    " 'pmStress',\n",
    " 'pmFatigue',\n",
    " 'wakeupduration',\n",
    " 'lightsleepduration',\n",
    " 'deepsleepduration',\n",
    " 'wakeupcount',\n",
    " 'durationtosleep',\n",
    " 'remsleepduration',\n",
    " 'durationtowakeup',\n",
    " 'hr_average',\n",
    " 'rr_average',\n",
    " 'breathing_disturbances_intensity',\n",
    " 'snoring',\n",
    " 'snoringepisodecount',\n",
    " 'sleep_score',\n",
    " 'action_care_housemem',\n",
    " 'action_community_interaction',\n",
    " 'action_entertainment',\n",
    " 'action_hobby',\n",
    " 'action_household',\n",
    " 'action_meal',\n",
    " 'action_outdoor_act',\n",
    " 'action_personal_care',\n",
    " 'action_recreation_etc',\n",
    " 'action_recreation_media',\n",
    " 'action_shop',\n",
    " 'action_sleep',\n",
    " 'action_socialising',\n",
    " 'action_study',\n",
    " 'action_travel',\n",
    " 'action_work',\n",
    " 'condition_ALONE',\n",
    " 'condition_WITH_MANY',\n",
    " 'condition_WITH_ONE',\n",
    " 'place_home',\n",
    " 'place_other_indoor',\n",
    " 'place_outdoor',\n",
    " 'place_restaurant',\n",
    " 'place_workplace',\n",
    " 'activity_IN_VEHICLE',\n",
    " 'activity_ON_FOOT',\n",
    " 'activity_STILL',\n",
    " 'activity_UNKNOWN',\n",
    " 'caffeine_caffeinated drink',\n",
    " 'caffeine_coffee',\n",
    " 'caffeine_coke',\n",
    " 'caffeine_tea',\n",
    " 'caffeine_unknown',\n",
    " 'alcohol_beer',\n",
    " 'alcohol_beer&rice wine',\n",
    " 'alcohol_not specified',\n",
    " 'alcohol_soju',\n",
    " 'alcohol_soju&beer',\n",
    " 'alcohol_unknown',\n",
    " 'alcohol_wine',\n",
    " 'cAmount(ml)_100.0',\n",
    " 'cAmount(ml)_150.0',\n",
    " 'cAmount(ml)_200.0',\n",
    " 'cAmount(ml)_250.0',\n",
    " 'cAmount(ml)_260.0',\n",
    " 'cAmount(ml)_280.0',\n",
    " 'cAmount(ml)_300.0',\n",
    " 'cAmount(ml)_350.0',\n",
    " 'cAmount(ml)_355.0',\n",
    " 'cAmount(ml)_360.0',\n",
    " 'cAmount(ml)_400.0',\n",
    " 'cAmount(ml)_450.0',\n",
    " 'cAmount(ml)_500.0',\n",
    " 'cAmount(ml)_560.0',\n",
    " 'cAmount(ml)_600.0',\n",
    " 'cAmount(ml)_700.0',\n",
    " 'cAmount(ml)_750.0',\n",
    " 'cAmount(ml)_900.0',\n",
    " 'cAmount(ml)_1000.0',\n",
    " 'cAmount(ml)_1500.0',\n",
    " 'cAmount(ml)_unknown',\n",
    " 'aAmount(ml)_200.0',\n",
    " 'aAmount(ml)_250.0',\n",
    " 'aAmount(ml)_300.0',\n",
    " 'aAmount(ml)_330.0',\n",
    " 'aAmount(ml)_400.0',\n",
    " 'aAmount(ml)_500.0',\n",
    " 'aAmount(ml)_600.0',\n",
    " 'aAmount(ml)_700.0',\n",
    " 'aAmount(ml)_720.0',\n",
    " 'aAmount(ml)_750.0',\n",
    " 'aAmount(ml)_800.0',\n",
    " 'aAmount(ml)_900.0',\n",
    " 'aAmount(ml)_1000.0',\n",
    " 'aAmount(ml)_1500.0',\n",
    " 'aAmount(ml)_2000.0',\n",
    " 'aAmount(ml)_3000.0',\n",
    " 'aAmount(ml)_3500.0',\n",
    " 'aAmount(ml)_4000.0',\n",
    " 'aAmount(ml)_unknown']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize X_missing\n",
    "col_order = X_train_scaled.columns.tolist()\n",
    "#X_missing.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "numeric_cols = ['heart_rate', 'latitude', 'longitude', 'magnitude_mAcc']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_missing_numeric_scaled = scaler.fit_transform(X_missing[numeric_cols])\n",
    "# Convert back to df\n",
    "X_missing_numeric_scaled_df = pd.DataFrame(X_missing_numeric_scaled, columns=numeric_cols, index=X_missing.index)\n",
    "# Combine scaled numeric columns with categorical columns\n",
    "categorical_df = X_missing.drop(columns=['heart_rate', 'latitude', 'longitude', 'magnitude_mAcc'])\n",
    "X_missing_scaled = pd.concat([X_missing_numeric_scaled_df, categorical_df], axis=1)\n",
    "X_missing_scaled = X_missing_scaled[col_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Impute validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "\n",
    "# Initialize the IterativeImputer\n",
    "imputer = IterativeImputer(estimator=BayesianRidge(), random_state=0, max_iter=20, sample_posterior=True)\n",
    "\n",
    "# Fit the imputer on the training data\n",
    "imputer.fit(X_train_scaled)\n",
    "\n",
    "# Transform the data with missing values\n",
    "X_imputed = imputer.transform(X_missing_scaled)\n",
    "\n",
    "# X_imputed now has the missing values imputed based on the learned model from X_train\n",
    "\n",
    "validation_imputed = pd.DataFrame(X_imputed, columns=X_missing.columns)\n",
    "\n",
    "with open('imputed_validation.pickle', 'wb') as f:\n",
    "    pickle.dump(validation_imputed, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Visualize X_train_scaled and validation_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = X_train_scaled.shape[1]  # Number of variables (columns) in X_train_scaled\n",
    "fig, axes = plt.subplots(nrows=num_vars, ncols=1, figsize=(8, 4 * num_vars))\n",
    "\n",
    "# Loop through each variable\n",
    "for i, var in enumerate(X_train_scaled.columns):\n",
    "    ax = axes[i] if num_vars > 1 else axes  # Select the appropriate subplot\n",
    "    sns.histplot(X_train_scaled[var], kde=True, color='blue', label=f'X_train_scaled {var}', ax=ax)\n",
    "    sns.histplot(validation_imputed[var], kde=True, color='red', label=f'validation_imputed {var}', ax=ax)\n",
    "    ax.set_title(f'Distribution of {var}')\n",
    "    ax.legend()\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Impute test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./imputed_train_with_labels.pickle', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "\n",
    "with open('./test.pickle', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "# Separate features and target labels\n",
    "train_features = train_data.drop(columns=[col for col in train_data.columns if col.startswith('daily_')])\n",
    "train_features.drop(columns=['userId', 'date', 'timestamp'], inplace=True)\n",
    "\n",
    "test_features = test_data.drop(columns=[col for col in test_data.columns if col.startswith('daily_')])\n",
    "test_features.drop(columns=['subject_id', 'date', 'timestamp'], inplace=True)\n",
    "test = set(test_features.columns)\n",
    "train = set(train_features.columns)\n",
    "in_test_not_in_train = test - train\n",
    "only_in_test = list(in_test_not_in_train)\n",
    "#print(f\"In test not in train: {in_test_not_in_train}, In train not in test: {in_train_not_in_test}\")\n",
    "\n",
    "prefixes = ['action', 'condition', 'place', 'activity', 'caffeine', 'alcohol', 'cAmount(ml)', 'aAmount(ml)']\n",
    "onehot_cols = [col for col in train_features.columns if any(col.startswith(prefix) for prefix in prefixes)]\n",
    "# Identify numeric columns (those that are not one-hot encoded)\n",
    "numeric_cols = train_features.columns.difference(onehot_cols).tolist()\n",
    "\n",
    "# Identify features that are missing in the test set\n",
    "# Assuming features is the DataFrame used for training which contains all columns\n",
    "missing_features = [col for col in train_features.columns if col not in test_features.columns]\n",
    "\n",
    "# Append the missing features to the test set with placeholder testues (e.g., NaN)\n",
    "for feature in missing_features:\n",
    "    test_features[feature] = float('nan')\n",
    "\n",
    "X_missing_test = test_features.drop(columns=only_in_test)\n",
    "for feature in missing_features:\n",
    "    X_missing_test[feature] = float('nan')\n",
    "\n",
    "X_missing_test = X_missing_test[['heart_rate',\n",
    " 'magnitude_mAcc',\n",
    " 'bvp_positive',\n",
    " 'bvp_negative',\n",
    " 'temp',\n",
    " 'magnitude_e4Acc',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'accuracy',\n",
    " 'magnitude_mMag',\n",
    " 'eda',\n",
    " 'magnitude_mGyr',\n",
    " 'emotionPositive',\n",
    " 'emotionTension',\n",
    " 'sleep',\n",
    " 'sleepProblem',\n",
    " 'dream',\n",
    " 'amCondition',\n",
    " 'amEmotion',\n",
    " 'pmEmotion',\n",
    " 'pmStress',\n",
    " 'pmFatigue',\n",
    " 'wakeupduration',\n",
    " 'lightsleepduration',\n",
    " 'deepsleepduration',\n",
    " 'wakeupcount',\n",
    " 'durationtosleep',\n",
    " 'remsleepduration',\n",
    " 'durationtowakeup',\n",
    " 'hr_average',\n",
    " 'rr_average',\n",
    " 'breathing_disturbances_intensity',\n",
    " 'snoring',\n",
    " 'snoringepisodecount',\n",
    " 'sleep_score',\n",
    " 'action_care_housemem',\n",
    " 'action_community_interaction',\n",
    " 'action_entertainment',\n",
    " 'action_hobby',\n",
    " 'action_household',\n",
    " 'action_meal',\n",
    " 'action_outdoor_act',\n",
    " 'action_personal_care',\n",
    " 'action_recreation_etc',\n",
    " 'action_recreation_media',\n",
    " 'action_shop',\n",
    " 'action_sleep',\n",
    " 'action_socialising',\n",
    " 'action_study',\n",
    " 'action_travel',\n",
    " 'action_work',\n",
    " 'condition_ALONE',\n",
    " 'condition_WITH_MANY',\n",
    " 'condition_WITH_ONE',\n",
    " 'place_home',\n",
    " 'place_other_indoor',\n",
    " 'place_outdoor',\n",
    " 'place_restaurant',\n",
    " 'place_workplace',\n",
    " 'activity_IN_VEHICLE',\n",
    " 'activity_ON_FOOT',\n",
    " 'activity_STILL',\n",
    " 'activity_UNKNOWN',\n",
    " 'caffeine_caffeinated drink',\n",
    " 'caffeine_coffee',\n",
    " 'caffeine_coke',\n",
    " 'caffeine_tea',\n",
    " 'caffeine_unknown',\n",
    " 'alcohol_beer',\n",
    " 'alcohol_beer&rice wine',\n",
    " 'alcohol_not specified',\n",
    " 'alcohol_soju',\n",
    " 'alcohol_soju&beer',\n",
    " 'alcohol_unknown',\n",
    " 'alcohol_wine',\n",
    " 'cAmount(ml)_100.0',\n",
    " 'cAmount(ml)_150.0',\n",
    " 'cAmount(ml)_200.0',\n",
    " 'cAmount(ml)_250.0',\n",
    " 'cAmount(ml)_260.0',\n",
    " 'cAmount(ml)_280.0',\n",
    " 'cAmount(ml)_300.0',\n",
    " 'cAmount(ml)_350.0',\n",
    " 'cAmount(ml)_355.0',\n",
    " 'cAmount(ml)_360.0',\n",
    " 'cAmount(ml)_400.0',\n",
    " 'cAmount(ml)_450.0',\n",
    " 'cAmount(ml)_500.0',\n",
    " 'cAmount(ml)_560.0',\n",
    " 'cAmount(ml)_600.0',\n",
    " 'cAmount(ml)_700.0',\n",
    " 'cAmount(ml)_750.0',\n",
    " 'cAmount(ml)_900.0',\n",
    " 'cAmount(ml)_1000.0',\n",
    " 'cAmount(ml)_1500.0',\n",
    " 'cAmount(ml)_unknown',\n",
    " 'aAmount(ml)_200.0',\n",
    " 'aAmount(ml)_250.0',\n",
    " 'aAmount(ml)_300.0',\n",
    " 'aAmount(ml)_330.0',\n",
    " 'aAmount(ml)_400.0',\n",
    " 'aAmount(ml)_500.0',\n",
    " 'aAmount(ml)_600.0',\n",
    " 'aAmount(ml)_700.0',\n",
    " 'aAmount(ml)_720.0',\n",
    " 'aAmount(ml)_750.0',\n",
    " 'aAmount(ml)_800.0',\n",
    " 'aAmount(ml)_900.0',\n",
    " 'aAmount(ml)_1000.0',\n",
    " 'aAmount(ml)_1500.0',\n",
    " 'aAmount(ml)_2000.0',\n",
    " 'aAmount(ml)_3000.0',\n",
    " 'aAmount(ml)_3500.0',\n",
    " 'aAmount(ml)_4000.0',\n",
    " 'aAmount(ml)_unknown']]\n",
    "\n",
    "\n",
    "#X_missing.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "numeric_cols = ['heart_rate', 'latitude', 'longitude', 'magnitude_mAcc']\n",
    "col_order = X_train_scaled.columns.tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_missing_test_numeric_scaled = scaler.fit_transform(X_missing_test[numeric_cols])\n",
    "# Convert back to df\n",
    "X_missing_test_numeric_scaled_df = pd.DataFrame(X_missing_test_numeric_scaled, columns=numeric_cols, index=X_missing_test.index)\n",
    "\n",
    "# Combine scaled numeric columns with categorical columns\n",
    "categorical_df = X_missing_test.drop(columns=['heart_rate', 'latitude', 'longitude', 'magnitude_mAcc'])\n",
    "X_missing_test_scaled = pd.concat([X_missing_test_numeric_scaled_df, categorical_df], axis=1)\n",
    "X_missing_test_scaled = X_missing_test_scaled[col_order]\n",
    "\n",
    "\n",
    "# Assuming X_train_scaled is your training data without missing values\n",
    "# Assuming X_test_missing is your test data with missing values\n",
    "\n",
    "# Initialize the IterativeImputer\n",
    "imputer = IterativeImputer(estimator=BayesianRidge(), random_state=0, max_iter=20, sample_posterior=True)\n",
    "\n",
    "# Fit the imputer on the training data\n",
    "imputer.fit(X_train_scaled)\n",
    "\n",
    "# Transform the data with missing values\n",
    "X_imputed = imputer.transform(X_missing_test_scaled)\n",
    "\n",
    "test_imputed = pd.DataFrame(X_imputed, columns=X_missing_test.columns)\n",
    "\n",
    "with open('imputed_test.pickle', 'wb') as f:\n",
    "    pickle.dump(test_imputed, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two-stage modeling\n",
    "\n",
    "- Set 1: Features only in the training set.\n",
    "- Set 2: Features shared by the training and validation sets.\n",
    "- Set 3: Features only in the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1_features = list(X_train_scaled.columns)  # same as list(validation_imputed.columns)\n",
    "set2_features = list(X_train_scaled.columns)  # same as list(validation_imputed.columns)\n",
    "set3_features = ['highest_prob_sound', 'altitude', 'speed_x', 'speed_y', 'm_light', 'total_time_sum', 'burned_calories', 'distance', 'running_steps', 'steps', 'step_frequency', 'walking_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1_features = ['heart_rate',\n",
    " 'magnitude_mAcc',\n",
    " 'bvp_positive',\n",
    " 'bvp_negative',\n",
    " 'temp',\n",
    " 'magnitude_e4Acc',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'accuracy',\n",
    " 'magnitude_mMag',\n",
    " 'eda',\n",
    " 'magnitude_mGyr',\n",
    " 'emotionPositive',\n",
    " 'emotionTension',\n",
    " 'sleep',\n",
    " 'sleepProblem',\n",
    " 'dream',\n",
    " 'amCondition',\n",
    " 'amEmotion',\n",
    " 'pmEmotion',\n",
    " 'pmStress',\n",
    " 'pmFatigue',\n",
    " 'wakeupduration',\n",
    " 'lightsleepduration',\n",
    " 'deepsleepduration',\n",
    " 'wakeupcount',\n",
    " 'durationtosleep',\n",
    " 'remsleepduration',\n",
    " 'durationtowakeup',\n",
    " 'hr_average',\n",
    " 'rr_average',\n",
    " 'breathing_disturbances_intensity',\n",
    " 'snoring',\n",
    " 'snoringepisodecount',\n",
    " 'sleep_score',\n",
    " 'action_care_housemem',\n",
    " 'action_community_interaction',\n",
    " 'action_entertainment',\n",
    " 'action_hobby',\n",
    " 'action_household',\n",
    " 'action_meal',\n",
    " 'action_outdoor_act',\n",
    " 'action_personal_care',\n",
    " 'action_recreation_etc',\n",
    " 'action_recreation_media',\n",
    " 'action_shop',\n",
    " 'action_sleep',\n",
    " 'action_socialising',\n",
    " 'action_study',\n",
    " 'action_travel',\n",
    " 'action_work',\n",
    " 'condition_ALONE',\n",
    " 'condition_WITH_MANY',\n",
    " 'condition_WITH_ONE',\n",
    " 'place_home',\n",
    " 'place_other_indoor',\n",
    " 'place_outdoor',\n",
    " 'place_restaurant',\n",
    " 'place_workplace',\n",
    " 'activity_IN_VEHICLE',\n",
    " 'activity_ON_FOOT',\n",
    " 'activity_STILL',\n",
    " 'activity_UNKNOWN',\n",
    " 'caffeine_caffeinated drink',\n",
    " 'caffeine_coffee',\n",
    " 'caffeine_coke',\n",
    " 'caffeine_tea',\n",
    " 'caffeine_unknown',\n",
    " 'alcohol_beer',\n",
    " 'alcohol_beer&rice wine',\n",
    " 'alcohol_not specified',\n",
    " 'alcohol_soju',\n",
    " 'alcohol_soju&beer',\n",
    " 'alcohol_unknown',\n",
    " 'alcohol_wine',\n",
    " 'cAmount(ml)_100.0',\n",
    " 'cAmount(ml)_150.0',\n",
    " 'cAmount(ml)_200.0',\n",
    " 'cAmount(ml)_250.0',\n",
    " 'cAmount(ml)_260.0',\n",
    " 'cAmount(ml)_280.0',\n",
    " 'cAmount(ml)_300.0',\n",
    " 'cAmount(ml)_350.0',\n",
    " 'cAmount(ml)_355.0',\n",
    " 'cAmount(ml)_360.0',\n",
    " 'cAmount(ml)_400.0',\n",
    " 'cAmount(ml)_450.0',\n",
    " 'cAmount(ml)_500.0',\n",
    " 'cAmount(ml)_560.0',\n",
    " 'cAmount(ml)_600.0',\n",
    " 'cAmount(ml)_700.0',\n",
    " 'cAmount(ml)_750.0',\n",
    " 'cAmount(ml)_900.0',\n",
    " 'cAmount(ml)_1000.0',\n",
    " 'cAmount(ml)_1500.0',\n",
    " 'cAmount(ml)_unknown',\n",
    " 'aAmount(ml)_200.0',\n",
    " 'aAmount(ml)_250.0',\n",
    " 'aAmount(ml)_300.0',\n",
    " 'aAmount(ml)_330.0',\n",
    " 'aAmount(ml)_400.0',\n",
    " 'aAmount(ml)_500.0',\n",
    " 'aAmount(ml)_600.0',\n",
    " 'aAmount(ml)_700.0',\n",
    " 'aAmount(ml)_720.0',\n",
    " 'aAmount(ml)_750.0',\n",
    " 'aAmount(ml)_800.0',\n",
    " 'aAmount(ml)_900.0',\n",
    " 'aAmount(ml)_1000.0',\n",
    " 'aAmount(ml)_1500.0',\n",
    " 'aAmount(ml)_2000.0',\n",
    " 'aAmount(ml)_3000.0',\n",
    " 'aAmount(ml)_3500.0',\n",
    " 'aAmount(ml)_4000.0',\n",
    " 'aAmount(ml)_unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set2_features = ['heart_rate',\n",
    " 'magnitude_mAcc',\n",
    " 'bvp_positive',\n",
    " 'bvp_negative',\n",
    " 'temp',\n",
    " 'magnitude_e4Acc',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'accuracy',\n",
    " 'magnitude_mMag',\n",
    " 'eda',\n",
    " 'magnitude_mGyr',\n",
    " 'emotionPositive',\n",
    " 'emotionTension',\n",
    " 'sleep',\n",
    " 'sleepProblem',\n",
    " 'dream',\n",
    " 'amCondition',\n",
    " 'amEmotion',\n",
    " 'pmEmotion',\n",
    " 'pmStress',\n",
    " 'pmFatigue',\n",
    " 'wakeupduration',\n",
    " 'lightsleepduration',\n",
    " 'deepsleepduration',\n",
    " 'wakeupcount',\n",
    " 'durationtosleep',\n",
    " 'remsleepduration',\n",
    " 'durationtowakeup',\n",
    " 'hr_average',\n",
    " 'rr_average',\n",
    " 'breathing_disturbances_intensity',\n",
    " 'snoring',\n",
    " 'snoringepisodecount',\n",
    " 'sleep_score',\n",
    " 'action_care_housemem',\n",
    " 'action_community_interaction',\n",
    " 'action_entertainment',\n",
    " 'action_hobby',\n",
    " 'action_household',\n",
    " 'action_meal',\n",
    " 'action_outdoor_act',\n",
    " 'action_personal_care',\n",
    " 'action_recreation_etc',\n",
    " 'action_recreation_media',\n",
    " 'action_shop',\n",
    " 'action_sleep',\n",
    " 'action_socialising',\n",
    " 'action_study',\n",
    " 'action_travel',\n",
    " 'action_work',\n",
    " 'condition_ALONE',\n",
    " 'condition_WITH_MANY',\n",
    " 'condition_WITH_ONE',\n",
    " 'place_home',\n",
    " 'place_other_indoor',\n",
    " 'place_outdoor',\n",
    " 'place_restaurant',\n",
    " 'place_workplace',\n",
    " 'activity_IN_VEHICLE',\n",
    " 'activity_ON_FOOT',\n",
    " 'activity_STILL',\n",
    " 'activity_UNKNOWN',\n",
    " 'caffeine_caffeinated drink',\n",
    " 'caffeine_coffee',\n",
    " 'caffeine_coke',\n",
    " 'caffeine_tea',\n",
    " 'caffeine_unknown',\n",
    " 'alcohol_beer',\n",
    " 'alcohol_beer&rice wine',\n",
    " 'alcohol_not specified',\n",
    " 'alcohol_soju',\n",
    " 'alcohol_soju&beer',\n",
    " 'alcohol_unknown',\n",
    " 'alcohol_wine',\n",
    " 'cAmount(ml)_100.0',\n",
    " 'cAmount(ml)_150.0',\n",
    " 'cAmount(ml)_200.0',\n",
    " 'cAmount(ml)_250.0',\n",
    " 'cAmount(ml)_260.0',\n",
    " 'cAmount(ml)_280.0',\n",
    " 'cAmount(ml)_300.0',\n",
    " 'cAmount(ml)_350.0',\n",
    " 'cAmount(ml)_355.0',\n",
    " 'cAmount(ml)_360.0',\n",
    " 'cAmount(ml)_400.0',\n",
    " 'cAmount(ml)_450.0',\n",
    " 'cAmount(ml)_500.0',\n",
    " 'cAmount(ml)_560.0',\n",
    " 'cAmount(ml)_600.0',\n",
    " 'cAmount(ml)_700.0',\n",
    " 'cAmount(ml)_750.0',\n",
    " 'cAmount(ml)_900.0',\n",
    " 'cAmount(ml)_1000.0',\n",
    " 'cAmount(ml)_1500.0',\n",
    " 'cAmount(ml)_unknown',\n",
    " 'aAmount(ml)_200.0',\n",
    " 'aAmount(ml)_250.0',\n",
    " 'aAmount(ml)_300.0',\n",
    " 'aAmount(ml)_330.0',\n",
    " 'aAmount(ml)_400.0',\n",
    " 'aAmount(ml)_500.0',\n",
    " 'aAmount(ml)_600.0',\n",
    " 'aAmount(ml)_700.0',\n",
    " 'aAmount(ml)_720.0',\n",
    " 'aAmount(ml)_750.0',\n",
    " 'aAmount(ml)_800.0',\n",
    " 'aAmount(ml)_900.0',\n",
    " 'aAmount(ml)_1000.0',\n",
    " 'aAmount(ml)_1500.0',\n",
    " 'aAmount(ml)_2000.0',\n",
    " 'aAmount(ml)_3000.0',\n",
    " 'aAmount(ml)_3500.0',\n",
    " 'aAmount(ml)_4000.0',\n",
    " 'aAmount(ml)_unknown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full validation set\n",
    "# concat validation_imputed (which has the same columns as train set) and the original validation set\n",
    "with open('./validation_with_labels.pickle', 'rb') as f:\n",
    "    validation_data = pickle.load(f)\n",
    "\n",
    "with open('./imputed_validation.pickle', 'rb') as f:\n",
    "    validation_imputed = pickle.load(f)\n",
    "\n",
    "validation_features = validation_data.drop(columns=[col for col in validation_data.columns if col.startswith('daily_')])\n",
    "validation_data = pd.concat([validation_features, validation_imputed], axis=1)\n",
    "validation_data = validation_data.drop(columns=['activity', 'subject_id', 'date', 'timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 1: Train an initial model on the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initial model training (features used: set 2)\n",
    "- Train the initial model using only the features available in the training set.\n",
    "- This model will be used to generate predictions that will serve as additional features in the second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_stage1 = RandomForestClassifier()\n",
    "model_stage1.fit(X_train_scaled[set2_features], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate predictions for validation set\n",
    "- Use the trained model (model_stage1) to predict on the validation set.\n",
    "- These predictions will act as an additional feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions_stage1 = model_stage1.predict_proba(validation_imputed[set2_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Add predictions to the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add stage1 predictions to the validation set\n",
    "\n",
    "# Check the shape of the predictions\n",
    "shapes = [array.shape for array in val_predictions_stage1]\n",
    "for i, shape in enumerate(shapes):\n",
    "    print(f\"Shape of array {i+1}: {shape}\")\n",
    "\n",
    "# Ensure each prediction array matches the number of rows in validation_imputed\n",
    "for i, preds in enumerate(val_predictions_stage1):\n",
    "    if len(preds) != validation_imputed.shape[0]:\n",
    "        raise ValueError(f\"Number of predictions for output {i+1} does not match the number of rows in validation_imputed\")\n",
    "\n",
    "# Add predictions for each output to the validation set\n",
    "for i, preds in enumerate(val_predictions_stage1):\n",
    "    validation_imputed[f'stage1_predictions_{i+1}'] = preds[:, 1]  #probability of the positive class\n",
    "\n",
    "validation_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stage 2: Train a second model using the predictions and additional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Train second model\n",
    "- Train a new model using both the original features and the additional features, along with the predictions from the first stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_stage2 = set2_features + set3_features + [f\"stage1_predictions_{i+1}\" for i in range(len(val_predictions_stage1))]\n",
    "validation_data_stage2 = validation_imputed.reindex(columns=features_stage2)\n",
    "model_stage2 = RandomForestClassifier()\n",
    "model_stage2.fit(validation_data_stage2, validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Evaluate and predict on test set\n",
    "- Use the first model to generate predictions for the test set.\n",
    "- Combine these predictions with the original features and the additional features of the test set to form the input for the second model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set using the first stage model\n",
    "test_predictions_stage1 = model_stage1.predict_proba(test_imputed[set2_features])\n",
    "\n",
    "# Add stage1 predictions to the test set for each output\n",
    "for i, preds in enumerate(test_predictions_stage1):\n",
    "    test_imputed[f\"stage1_predictions_{i+1}\"] = preds[:,1] # exact probability of the positive class\n",
    "\n",
    "\n",
    "# Ensure the test data includes the shared features, additional features, and stage1 predictions\n",
    "test_data_stage2 = test_imputed.reindex(columns=features_stage2)\n",
    "\n",
    "# Make final predictions on the test set\n",
    "final_test_predictions = model_stage2.predict(test_data_stage2)\n",
    "final_test_predictions_df = pd.DataFrame(final_test_predictions, columns=validation_labels.columns)\n",
    "final_test_predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./test.pickle', 'rb') as f:\n",
    "    test_data = pickle.load(f)\n",
    "\n",
    "# Separate features and target labels\n",
    "test_features = test_data.drop(columns=[col for col in test_data.columns if col.startswith('daily_')])\n",
    "test_features_append = test_features[['subject_id', 'date']]\n",
    "test_features_append\n",
    "final_test_predictions_df\n",
    "submit = pd.concat([test_features_append, final_test_predictions_df], axis=1)\n",
    "submit.to_csv(\"submit_0625.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
