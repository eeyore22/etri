{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import math\n",
    "from pandas.errors import EmptyDataError\n",
    "import pickle\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For renaming the filenames in 'user_processed' folder from Unix epoch time to datetime'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unix_to_datetime(unix_time):\n",
    "    return datetime.utcfromtimestamp(unix_time).strftime('%Y-%m-%d')\n",
    "\n",
    "parent_folder = './train_dataset/user21-25/user25_processed'\n",
    "\n",
    "for subfolder in os.listdir(parent_folder):\n",
    "    subfolder_path = os.path.join(parent_folder, subfolder)\n",
    "    # check if it's a directory \n",
    "    if os.path.isdir(subfolder_path):\n",
    "        try:\n",
    "            # convert subfolder name to datetime format\n",
    "            new_name = unix_to_datetime(int(subfolder))\n",
    "            os.rename(subfolder_path, os.path.join(parent_folder, new_name))\n",
    "            print(f\"Renamed {subfolder} to {new_name}\")\n",
    "\n",
    "            csv_file = os.path.join(parent_folder, new_name, f\"{subfolder}_label.csv\")\n",
    "            new_csv_name = os.path.join(parent_folder, new_name, f\"{new_name}_label.csv\")\n",
    "\n",
    "            os.rename(csv_file, new_csv_name)\n",
    "            print(f\"Renamed {csv_file} to {new_csv_name}\")\n",
    "\n",
    "        except ValueError:\n",
    "            print(f\"Skipping {subfolder}: Not a valid Unix epoch time\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error renaming {subfolder}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert Unix epoch time to datetime format\n",
    "def unix_to_datetime(unix_time):\n",
    "    return datetime.utcfromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "# Path to the folder containing CSV files with Unix epoch filenames\n",
    "folder_path = './train_dataset/user01-06/user01_processed/2020-08-31/e4Acc'\n",
    "\n",
    "# Iterate through each file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    # Check if the file is a CSV file\n",
    "    if file_name.endswith('.csv'):\n",
    "        try:\n",
    "            # Extract Unix epoch time from the filename\n",
    "            epoch_time = int(os.path.splitext(file_name)[0])\n",
    "            \n",
    "            # Convert Unix epoch time to datetime format\n",
    "            new_name = unix_to_datetime(epoch_time)\n",
    "            \n",
    "            # Rename the file with the new datetime format\n",
    "            old_path = os.path.join(folder_path, file_name)\n",
    "            new_path = os.path.join(folder_path, f\"{new_name}.csv\")\n",
    "            os.rename(old_path, new_path)\n",
    "            \n",
    "            print(f\"Renamed {file_name} to {new_name}.csv\")\n",
    "        except ValueError:\n",
    "            print(f\"Skipping {file_name}: Not a valid Unix epoch time\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error renaming {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_pickle(filename: str):\n",
    "    with open(filename, 'rb') as file:\n",
    "        load = pickle.load(file)\n",
    "    return load\n",
    "\n",
    "def save_pickle(filename: str, data):\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data aggregation\n",
    "- Originally, for each user, date -> sensor data name -> separate csv files for each 'minute'.\n",
    "- To aggregate, first produce a single file for each date's sensor data name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, add the date ('2020-08-30') and time in seconds ('36') to create a column 'timestamp' in ('%Y-%m-%d %H:%M:%S.%f') format\n",
    "# then combine them for each sensor type\n",
    "\n",
    "# Function to convert Unix time to datetime string\n",
    "def unix_to_datetime(unix_time):\n",
    "    return datetime.utcfromtimestamp(unix_time).strftime('%Y-%m-%d %H:%M:%S.%f')\n",
    "\n",
    "def calculate_magnitude(x, y, z):\n",
    "    return math.sqrt(x**2 + y**2 + z**2)\n",
    "\n",
    "# Function to reformat sensor data in a directory\n",
    "def reformat_sensor_data(date_directory, data_type, output_dir):\n",
    "    directory = date_directory / data_type\n",
    "    output_filename = f\"{data_type}_{date_directory.name}_combined.csv\"\n",
    "    output_file = output_dir / output_filename\n",
    "\n",
    "    # Skip processing if the output file already exists\n",
    "    if output_file.exists():\n",
    "        print(f\"Processed file already exists: {output_file}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    csv_files = list(directory.glob('*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {directory}, skipping.\")\n",
    "        return None\n",
    "    \n",
    "    dfs = []\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file)\n",
    "        except EmptyDataError:\n",
    "            print(f\"EmptyDataError: {csv_file} is empty, skipping this file.\")\n",
    "            continue\n",
    "            \n",
    "        time_value = csv_file.stem\n",
    "        try:\n",
    "            df['time_value'] = float(time_value)\n",
    "        except ValueError:\n",
    "            print(f\"ValueError: could not convert {time_value} to float, skipping this file.\")\n",
    "            continue\n",
    "        dfs.append(df)\n",
    "    \n",
    "    if not dfs:\n",
    "        print(f\"No valid CSV files found in {directory}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    combined_df = combined_df.sort_values(['time_value', 'timestamp'], ascending=[True, True])\n",
    "    \n",
    "    def safe_float_conversion(x):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except ValueError:\n",
    "            print(f\"ValueError: could not convert {x} to float, skipping this row.\")\n",
    "            return None\n",
    "    \n",
    "    combined_df['timestamp'] = combined_df['timestamp'].apply(safe_float_conversion)\n",
    "    combined_df = combined_df.dropna(subset=['timestamp'])  # Drop rows where conversion failed\n",
    "    \n",
    "    combined_df['timestamp'] = (combined_df['time_value'] + combined_df['timestamp']).apply(unix_to_datetime)\n",
    "\n",
    "    final_df = combined_df.drop(columns=['time_value'])\n",
    "    \n",
    "    if data_type in ['mAcc', 'e4Acc', 'mGyr', 'mMag']:\n",
    "        # Apply calculate_magnitude function to specified columns\n",
    "        magnitude_col = f\"magnitude_{data_type}\"\n",
    "        final_df[magnitude_col] = final_df.apply(lambda row: calculate_magnitude(row['x'], row['y'], row['z']), axis=1)\n",
    "\n",
    "    # Save the final DataFrame to a CSV file\n",
    "    final_df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved processed data to: {output_file}\")\n",
    "    return final_df\n",
    "\n",
    "# Define the base directory and output directory\n",
    "base_dir = Path('./train_dataset/user01-06/user05_processed')\n",
    "output_dir = Path('./train_dataset/user01-06/user05_processed')\n",
    "\n",
    "# Traverse the directory structure and process each date directory\n",
    "for date_dir in base_dir.rglob('*'):\n",
    "    if date_dir.is_dir():\n",
    "        for data_type in ['e4Acc', 'e4Bvp', 'e4Eda', 'e4Hr', 'e4Temp', 'mAcc', 'mGps', 'mGyr', 'mMag']:\n",
    "            data_type_path = date_dir / data_type\n",
    "            if data_type_path.is_dir():\n",
    "                print(f\"Processing directory: {data_type_path}\")\n",
    "                final_df = reformat_sensor_data(date_dir, data_type, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_combined_files(directory):\n",
    "    files = os.listdir(directory)\n",
    "    for file in files:\n",
    "        if file.endswith(\"_combined.csv\"):\n",
    "            file_path = os.path.join(directory, file)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "\n",
    "directory_path = './train_dataset/user01-06/user06_processed'\n",
    "delete_combined_files(directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data type: e4Acc\n",
      "Concatenated data saved to: train_dataset/user01-06/user06_processed/user06_e4Acc_combined.csv\n",
      "Processing data type: e4Bvp\n",
      "Concatenated data saved to: train_dataset/user01-06/user06_processed/user06_e4Bvp_combined.csv\n",
      "Processing data type: e4Eda\n",
      "Concatenated data saved to: train_dataset/user01-06/user06_processed/user06_e4Eda_combined.csv\n",
      "Processing data type: e4Hr\n",
      "Concatenated data saved to: train_dataset/user01-06/user06_processed/user06_e4Hr_combined.csv\n",
      "Processing data type: e4Temp\n",
      "Concatenated data saved to: train_dataset/user01-06/user06_processed/user06_e4Temp_combined.csv\n",
      "Processing data type: mAcc\n",
      "Concatenated data saved to: train_dataset/user01-06/user06_processed/user06_mAcc_combined.csv\n",
      "Processing data type: mGps\n",
      "Concatenated data saved to: train_dataset/user01-06/user06_processed/user06_mGps_combined.csv\n",
      "Processing data type: mGyr\n",
      "Concatenated data saved to: train_dataset/user01-06/user06_processed/user06_mGyr_combined.csv\n",
      "Processing data type: mMag\n",
      "Concatenated data saved to: train_dataset/user01-06/user06_processed/user06_mMag_combined.csv\n"
     ]
    }
   ],
   "source": [
    "def concat_csv(data_type, output_dir):\n",
    "    files_to_concat = [file for file in output_dir.glob(\"*.csv\") if data_type in file.name]\n",
    "    if len(files_to_concat) <2:\n",
    "        print(f\"Not enough files found for {data_type}, skipping concatenation\")\n",
    "        return None \n",
    "    \n",
    "    dfs = [pd.read_csv(file) for file in files_to_concat]\n",
    "    combined_df = pd.concat(dfs, ignore_index=True)\n",
    "    combined_df = combined_df.sort_values('timestamp')\n",
    "    return combined_df \n",
    "\n",
    "output_dir = Path('./train_dataset/user01-06/user06_processed')\n",
    "user_id = 'user06'\n",
    "\n",
    "for data_type in ['e4Acc', 'e4Bvp', 'e4Eda', 'e4Hr', 'e4Temp', 'mAcc', 'mGps', 'mGyr', 'mMag']:\n",
    "    print(f\"Processing data type: {data_type}\")\n",
    "    concat_df = concat_csv(data_type, output_dir)\n",
    "    if concat_df is not None:\n",
    "        output_filename = f\"{user_id}_{data_type}_combined.csv\"\n",
    "        output_file = output_dir / output_filename\n",
    "        concat_df.to_csv(output_file, index=False)\n",
    "        print(f\"Concatenated data saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_data(df, unit: str):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "    resampled_df = df.resample(unit).mean()\n",
    "    return resampled_df\n",
    "\n",
    "\n",
    "def resample_data_bvp(df, unit: str):\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df.set_index('timestamp', inplace=True)\n",
    "\n",
    "    df['bvp_positive'] = df['value'].apply(lambda x: x if x > 0 else None)\n",
    "    df['bvp_negative'] = df['value'].apply(lambda x: x if x < 0 else None)\n",
    "    \n",
    "    positive_mean = df['bvp_positive'].resample(unit).mean()\n",
    "    negative_mean = df['bvp_negative'].resample(unit).mean()\n",
    "\n",
    "    resampled_df = pd.DataFrame({'bvp_positive': positive_mean, 'bvp_negative': negative_mean})\n",
    "\n",
    "    return resampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sensor_data(root, users, sensors, resample_freq):\n",
    "\n",
    "    # Define dictionary for resampling frequency\n",
    "    freq_dict = {'h': 'hourly', 'min': 'minutely', 'd': 'daily'}\n",
    "    \n",
    "    # Loop through users and sensors\n",
    "    for user in users:\n",
    "        for sensor in sensors:\n",
    "            # Construct file path\n",
    "            path = os.path.join(root, f\"{user}_{sensor}_combined.csv\")\n",
    "\n",
    "            if not os.path.exists(path):\n",
    "                print(f\"File not found for user {user} and sensor {sensor}. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Read data and select columns based on sensor type\n",
    "            if sensor == 'e4Acc':\n",
    "                columns = ['timestamp', 'magnitude_e4Acc']\n",
    "            elif sensor == 'e4Bvp':\n",
    "                columns = ['timestamp', 'value']\n",
    "                resample_func = resample_data_bvp  # Use specific resampling function for 'e4Bvp'\n",
    "            elif sensor == 'e4Eda':\n",
    "                columns = ['timestamp', 'eda']\n",
    "            elif sensor == 'e4Hr':\n",
    "                columns = ['timestamp', 'hr']\n",
    "            elif sensor == 'e4Temp':\n",
    "                columns = ['timestamp', 'temp']\n",
    "            elif sensor == 'mAcc':\n",
    "                columns = ['timestamp', 'magnitude_mAcc']\n",
    "            elif sensor == 'mGps':\n",
    "                columns = ['timestamp', 'lat', 'lon', 'accuracy']\n",
    "            elif sensor == 'mGyr':\n",
    "                columns = ['timestamp', 'magnitude_mGyr']\n",
    "            elif sensor == 'mMag':\n",
    "                columns = ['timestamp', 'magnitude_mMag']\n",
    "            else:\n",
    "                print(f\"Unknown sensor type: {sensor}\")\n",
    "                continue\n",
    "            \n",
    "            # Read data\n",
    "            df = pd.read_csv(path)[columns]\n",
    "            \n",
    "            # Resample data\n",
    "            if resample_freq in ['h', 'min', 'd']:\n",
    "                if sensor == 'e4Bvp':\n",
    "                    resampled_data = resample_func(df, resample_freq)  # Use specific resampling function for 'e4Bvp'\n",
    "                else:\n",
    "                    resampled_data = resample_data(df, resample_freq)\n",
    "                \n",
    "                # Update CSV filename based on resampling frequency\n",
    "                csv_filename = f\"{user}_{sensor}_{freq_dict[resample_freq]}.csv\"\n",
    "                \n",
    "                # Export resampled data to CSV\n",
    "                df_path = \"./train_dataset/df\"\n",
    "                csv_path = os.path.join(df_path, csv_filename)\n",
    "                resampled_data.to_csv(csv_path)\n",
    "            else:\n",
    "                print(\"Invalid resampling frequency. Please use 'h', 'min', or 'd'.\")\n",
    "\n",
    "# Example usage\n",
    "root = './train_dataset/temp'\n",
    "users = ['user30']\n",
    "sensors = ['e4Acc', 'e4Bvp', 'e4Eda', 'e4Hr', 'e4Temp', 'mAcc', 'mGps', 'mGyr', 'mMag']\n",
    "resample_freq = 'h'  # Resampling frequency ('h', 'min', or 'd')\n",
    "\n",
    "process_sensor_data(root, users, sensors, resample_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1490808/1590121416.py:25: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grouped = df_sorted.groupby('timestamp').apply(lambda x: x.apply(first_non_nan)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = './train_dataset/df'\n",
    "\n",
    "# Function to find the first non-NaN value\n",
    "def first_non_nan(series):\n",
    "    return series.dropna().iloc[0] if not series.dropna().empty else np.nan\n",
    "\n",
    "# Initialize an empty list to hold dataframes\n",
    "dfs = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        # Read the CSV file into a dataframe and append to the list\n",
    "        dfs.append(pd.read_csv(file_path))\n",
    "\n",
    "\n",
    "df_concat = pd.concat(dfs, ignore_index=True)\n",
    "df_sorted = df_concat.sort_values('timestamp')\n",
    "df_grouped = df_sorted.groupby('timestamp').apply(lambda x: x.apply(first_non_nan)).reset_index(drop=True)\n",
    "df_grouped = df_grouped.reset_index()\n",
    "\n",
    "# Display the final dataframe\n",
    "df_grouped.to_csv(\"./train_dataset/sensor_data/hourly_remade/user30_hourly.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data for user01 saved to sensor_user01.csv\n",
      "Merged data for user06 saved to sensor_user06.csv\n",
      "Merged data for user05 saved to sensor_user05.csv\n",
      "Merged data for user03 saved to sensor_user03.csv\n",
      "Merged data for user04 saved to sensor_user04.csv\n",
      "Merged data for user02 saved to sensor_user02.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the directory containing the CSV files\n",
    "directory = './train_dataset/sensor_data/raw'\n",
    "\n",
    "# Regular expression pattern to extract the user substring (e.g., 'user01', 'user02')\n",
    "pattern = re.compile(r'user\\d{2}')\n",
    "\n",
    "# Dictionary to store lists of dataframes for each user substring\n",
    "dfs_dict = {}\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        match = pattern.search(filename)\n",
    "        if match:\n",
    "            user_substring = match.group(0)\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            if 'timestamp' in df.columns:\n",
    "                df.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            if user_substring not in dfs_dict:\n",
    "                dfs_dict[user_substring] = []\n",
    "            dfs_dict[user_substring].append(df)\n",
    "\n",
    "# Merge dataframes for each user substring and save to new CSV files\n",
    "for user_substring, dfs in dfs_dict.items():\n",
    "    if dfs:\n",
    "        # Concatenate dataframes on columns\n",
    "        merged_df = pd.concat(dfs, axis=1)\n",
    "        # Reset the index to make 'timestamp' a column again\n",
    "        merged_df.reset_index(inplace=True)\n",
    "        # Save the merged dataframe to a new CSV file\n",
    "        merged_filename = f'sensor_{user_substring}.csv'\n",
    "        merged_df.to_csv(os.path.join(directory, merged_filename), index=False)\n",
    "        print(f\"Merged data for {user_substring} saved to {merged_filename}\")\n",
    "    else:\n",
    "        print(f\"No CSV files found for {user_substring}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
